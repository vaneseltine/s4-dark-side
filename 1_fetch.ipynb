{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning\n",
    "\n",
    "https://docs.openalex.org/about-the-data/work\n",
    "\n",
    "So here's a process to build the data.\n",
    "\n",
    "1. Grab $S_r$ retracted articles: `is_retracted=true`, `type=research-article` ...minimum # of citations? ...pubmed ID?\n",
    "1. Grab $S_u$ unretracted articles as... random? (but how in API?) roughly matched* groups? 1:200? 1:1000?\n",
    "1. Repeat until sufficient data\n",
    "\n",
    "\\* Matching with...\n",
    "- `is_retracted=false`\n",
    "- `type=research-article`\n",
    "- Same year -- there are WAY more old unretracted articles in these data, and age matters a lot for citations\n",
    "- Matching on N concepts could help the data maintain a certain amount of comparability\n",
    "- Maybe limit to one area for that reason -- could require PubMed identifiers, which also might improve retraction measure\n",
    "- Same institution might helps avoid institutional confounds in data generation\n",
    "\n",
    "## Additional notes\n",
    "\n",
    "### Retraction Watch Database\n",
    "\n",
    "This sounds like the best source available, well curated and detailed, but it's not publicly available & requires a DUA with an organization for research. Probably best for a longer-term project, but not good for an 8 day deadline :)\n",
    "\n",
    "### open-retractions\n",
    "\n",
    "We could also hit up open-retractions API - https://github.com/open-retractions/open-retractions - using a DOI. The OpenAlex API documentation implies that they use Crossref but it's not clear whether they also use PubMed info for `is_retracted`; open-retractions does supply both CrossRef and PubMed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.8/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.8/site-packages (9.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.8/site-packages (from pyarrow) (1.19.5)\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /opt/conda/include/python3.8/UNKNOWN\n",
      "sysconfig: /opt/conda/include/python3.8\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# for i in range(10):\n",
    "#     clear_output(wait=True)\n",
    "#     print(i)\n",
    "#     time.sleep(0.1)\n",
    "\n",
    "DATA_DIR = Path('./data')\n",
    "UNRETRACTED_DIR = DATA_DIR / 'json_collecting'\n",
    "tester = DATA_DIR / 'test.txt'\n",
    "tester.touch()\n",
    "assert tester.exists()\n",
    "\n",
    "PER_PAGE = 200 # API max\n",
    "SHARED_FILTERS = \"type:journal-article,publication_year:>2010,publication_year:<2018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your own email here\n",
    "CONTACT_EMAIL = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have available a good chunk of retracted articles\n",
    "\n",
    "Retracted journal articles with at least 1 citation\n",
    "15,117 https://api.openalex.org/works?filter=type:journal-article,cited_by_count:>0&group_by=is_retracted\n",
    "\n",
    "Another \n",
    "5,415 with 0 citations https://api.openalex.org/works?filter=type:journal-article,cited_by_count:0&group_by=is_retracted\n",
    "\n",
    "Unfortunately, we don't have the date of retraction in these data.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = f\"https://api.openalex.org/works?mailto={CONTACT_EMAIL}\"\n",
    "filtering = f\"filter={SHARED_FILTERS},is_retracted:true\"\n",
    "pagination = f\"per_page={PER_PAGE}&page=1\"\n",
    "url = \"&\".join((base_url, filtering, pagination))\n",
    "j = requests.get(url).json()\n",
    "print(url)\n",
    "print(j['meta'])\n",
    "\n",
    "n_records = j['meta']['count']\n",
    "print('Records:', n_records)\n",
    "n_pages = math.ceil(n_records / PER_PAGE)\n",
    "print('Pages of records:', n_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_url = f\"https://api.openalex.org/works?mailto={CONTACT_EMAIL}\"\n",
    "filtering = f\"filter={SHARED_FILTERS},is_retracted:true\"\n",
    "\n",
    "records = []\n",
    "for i in range(1, 1000):\n",
    "    pagination = f\"per_page={PER_PAGE}&page={i}\"\n",
    "    url = \"&\".join((base_url, filtering, pagination))\n",
    "    j = requests.get(url).json()\n",
    "    print(url)\n",
    "    print(j['meta'])\n",
    "    \n",
    "    records += j['results']\n",
    "    \n",
    "    count = j['meta']['count']\n",
    "    page = j['meta']['page']\n",
    "    per_page = j['meta']['per_page']\n",
    "    if per_page * page >= count:\n",
    "        break\n",
    "    time.sleep(1)\n",
    "print('done')\n",
    "print(len(records))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json is only about 20% inflated over pkl\n",
    "with Path(DATA_DIR / 'raw_retracted.json').open('w', encoding='UTF-8') as outfile:\n",
    "    json.dump(records, outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to pull the  unretracted subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(DATA_DIR / 'raw_retracted.json').open('r', encoding='UTF-8') as infile:\n",
    "    raw_retracted_records = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journal_years = list(set((rec['host_venue']['id'], rec['publication_year']) for rec in raw_retracted_records))\n",
    "print(journal_years[:8])\n",
    "print(\"Retracted sample:\", len(raw_retracted_records))\n",
    "print(\"Unique year/journal combos:\", len(journal_years))\n",
    "print(\"Maximum possible unretracted sample:\", len(journal_years)*200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_url = f\"https://api.openalex.org/works?mailto={CONTACT_EMAIL}\"\n",
    "pagination = f\"per_page={PER_PAGE}&page=1\"\n",
    "\n",
    "if not UNRETRACTED_DIR.exists():\n",
    "    UNRETRACTED_DIR.mkdir()\n",
    "\n",
    "for i, journal_year in enumerate(journal_years):\n",
    "    clear_output(wait=True)\n",
    "    print(i+1, 'of', len(journal_years), journal_year)\n",
    "    \n",
    "    journal_url, year = journal_year\n",
    "    # Can't match the works with no venue id\n",
    "    if not journal_url:\n",
    "        continue\n",
    "    journal_id = journal_url.replace(\"https://openalex.org/\", \"\")\n",
    "\n",
    "    filename = UNRETRACTED_DIR / f\"{journal_id}-{year}.json\"\n",
    "    if filename.exists():\n",
    "        print(\"Already collected\", filename)\n",
    "        print('')\n",
    "        continue\n",
    "        \n",
    "    filtering = f\"filter=type:journal-article,is_retracted:false,host_venue.id:{journal_id},publication_year:{year}\"\n",
    "    url = \"&\".join((base_url, filtering, pagination))\n",
    "    print(url)\n",
    "    j = requests.get(url).json()\n",
    "    print(j['meta'])\n",
    "    \n",
    "    print('--->', filename)\n",
    "    with filename.open('w', encoding='UTF-8') as outfile:\n",
    "        json.dump(j, outfile)\n",
    "    \n",
    "    print('')\n",
    "    time.sleep(2)\n",
    "\n",
    "    \n",
    "print('done')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next:\n",
    "# validate each json as load()able\n",
    "# -- this will take a minute but will be much easier to refetch any errors rather than deal with the entire json\n",
    "\n",
    "for i, path in enumerate(UNRETRACTED_DIR.glob('*.json')):\n",
    "    clear_output(wait=True)\n",
    "    print(i+1, path)\n",
    "    with path.open('r', encoding='UTF-8') as infile:\n",
    "        json.load(infile)\n",
    "print('done')\n",
    "# if it doesn't error out, we're good\n",
    "# I'm seeing 5259 instead of 5260 because of the one missing host_venue.id"
   ]
  }
 ],
 "metadata": {
  "author": "me",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
